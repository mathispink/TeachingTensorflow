{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97fbd47e",
   "metadata": {},
   "source": [
    "# (Preprocessing) Tokenization\n",
    "\n",
    "In this notebook, you will learn how to do different kinds of tokenization using tensorflow-text and sentencepiece.\n",
    "\n",
    "Starting from character-based tokenization that takes individual characters as the tokens, we will have a look at word-based tokenization and its shortcomings, and then finally we'll investigate the benefits of sub-word tokenization (and how to do it) which is the most flexible tokenization that also works for any language (i.e. does not require words to be separated by white space etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a7e8e5",
   "metadata": {},
   "source": [
    "First we need to install tensorflow text, which will include some nice tokenizers that are used in modern NLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "304e1cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U tensorflow-text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b610db45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_text as tf_txt\n",
    "\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a791e937",
   "metadata": {},
   "source": [
    "We can use regular expressions and other operations such as string splitting for the tokenization (e.g. with the nltk library). The downside of this however is that the preprocessing will not be integratable with the model, complicating the data pipeline, model deployment and deteriorate performance since the preprocessing will not be part of the tensorflow graph.\n",
    "\n",
    "So instead, we want to use tokenizers that can be used inside a tensorflow model. The tensorflow-text library provides tools for both basic and advanced tokenization that can be integrated with a tensorflow model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44344fa9",
   "metadata": {},
   "source": [
    "## Character Tokenization\n",
    "\n",
    "Defining a vocabulary of all characters and then treating the text as a sequence of indices signifying the character used.\n",
    "\n",
    "Upsides:\n",
    "- Relatively small vocabulary\n",
    "\n",
    "Downsides:\n",
    "- Very long sequences (vanishing gradients, memory constraints, training and inference speed)\n",
    "- Quite unlike how humans read and understand text\n",
    "- Usually leads to worse performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1071f62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[ 79 117 116 112 117 116  32 111 102  32 108  97 110 103 117  97 103 101\n",
      "  32 109 111 100 101 108 115  32  99  97 110  32  98 101  32 100 101  99\n",
      " 101 112 116 105 118 101], shape=(42,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tf_txt.UnicodeCharTokenizer()\n",
    "tokenized_text = tokenizer.tokenize(\"Output of language models can be deceptive\")\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a7345e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'Output of language models can be deceptive', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "detokenized_text = tokenizer.detokenize(tokenized_text)\n",
    "print(detokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e46882d",
   "metadata": {},
   "source": [
    "## Word Tokenization\n",
    "\n",
    "Defining a vocabulary of all words that come up in the text. Treating the text as a sequence of indices signifying which word occurs.\n",
    "\n",
    "Upsides:\n",
    "- Only coherent words (that are part of the vocabulary) can be generated\n",
    "- Closer to how humans read and process text compared to a purely character-based representation\n",
    "\n",
    "Downsides:\n",
    "- Words can have many different versions, so some form of stemming would be needed to reduce vocabulary size\n",
    "- Large vocabulary size since there are a lot of words\n",
    "- Some tokens may be strongly under-represented in the data, \n",
    "- Words with the same sub-words are treated as independent from each other (in German, the word \"Bushaltestelle\" would be independent from the noun \"Bus\", the verb \"halten\" and the noun \"Stelle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa395041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"this\" \"is\" \"a\" \"sentence.\"]\n"
     ]
    }
   ],
   "source": [
    "tf.print(tf.strings.split(\"this is a sentence.\", sep=\" \",)  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da4139b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[b'This', b'is', b'some', b'text', b'.']]>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The better way to do it\n",
    "tokenizer = tf_txt.UnicodeScriptTokenizer()\n",
    "tokenizer.tokenize([\"This is some text.\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32211338",
   "metadata": {},
   "source": [
    "## Sentence Tokenization (generally a bad idea)\n",
    "\n",
    "Defining a vocabulary of all sentences that occur in the text. Treating the text as a sequence of indices signifying which sentence occurs.\n",
    "\n",
    "Upsides:\n",
    "\n",
    "- Only coherent sentences will be produced since the model can only output sentences that occured in the training data (which is hopefully but not naturally not garbage)\n",
    "- Sequences are much smaller, more text can be produced with less compute\n",
    "\n",
    "Downsides:\n",
    "- Depending on the text corpus, the vocabulary can be extremely large (combinatorial explosion)\n",
    "- Does not allow for new content\n",
    "- Not at all flexible - the slightest typo or difference in formatting compared to a sentence from the vocabulary will disable the model to process (and potentially comprehend) a sentence and force it to represent it as an \"unknown\" token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "355b82ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'there are so many possible sentences': 0,\n",
       " ' this is another sentence': 1,\n",
       " ' and here we have yet another sentence': 2,\n",
       " ' you would not believe it': 3,\n",
       " 'this is a sentence': 4}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_corpus = \"This is a sentence. This is another sentence. And here we have yet another sentence.\\\n",
    "There are so many possible sentences. You would not believe it.\".lower()\n",
    "\n",
    "# suboptimal splitting of sentences, preferably use a method from nltk etc.\n",
    "sentence_tokens = text_corpus.split(\".\") \n",
    "\n",
    "vocab_list = list(set(sentence_tokens))[1:]\n",
    "\n",
    "vocab = {}\n",
    "reverse_vocab = {}\n",
    "for i, element in enumerate(vocab_list):\n",
    "    vocab[element] = i\n",
    "    reverse_vocab[i] = element\n",
    "\n",
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28cd588",
   "metadata": {},
   "source": [
    "# Tokenization with subword segmentation (superior) \n",
    "\n",
    "Upsides:\n",
    "- Significantly smaller vocabulary size compared to word tokenization\n",
    "- Compositional relations between words can be learned with shared embeddings of sub-words\n",
    "- No stemming is needed to reduce vocabulary size since endings of verbs etc. are re-used for many words.\n",
    "- A strong language model could potentially produce new words or names by using sub-words in a meaningful way. (Spoiler alert: this typically does not happen with current models!)\n",
    "- Since there are multiple ways to segment words into subwords, this can be used for a regularizing effect on the pre-processing side.\n",
    "\n",
    "Downsides:\n",
    "- A model needs to be trained or an existing vocabulary of subwords needs to be found given a large corpus for the methods to work well\n",
    "- Grammatical mistakes can occur during learning, similar to character based tokenization (could also be regarded as an upside, depending on the interest in the language model)\n",
    "- Sequences get longer compared to word-level tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2432aa54",
   "metadata": {},
   "source": [
    "## Wordpiece Tokenizer\n",
    "\n",
    "https://paperswithcode.com/method/wordpiece\n",
    "\n",
    "The wordpiece tokenizer expects a text file containing a list of the possible sub-words.\n",
    "\n",
    "There is also a method called \"build_fast_wordpiece_model\" which supposedly allows to build a wordpiece vocabulary from raw text. However this is not documented, so we will only show how to use pre-defined vocabularies of sub-words. \n",
    "\n",
    "**Note that the wordpiece tokenizuer expects the text to already be split into words.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b5db59",
   "metadata": {},
   "source": [
    "First we download the pre-defined vocabulary of 7011 subwords.\n",
    "\n",
    "Some of the word pieces have \"##\" in them, meaning that the subword is a suffix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0bf6f61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://github.com/tensorflow/text/blob/master/tensorflow_text/python/ops/test_data/test_wp_en_vocab.txt?raw=true\"\n",
    "r = requests.get(url)\n",
    "filepath = \"vocab.txt\"\n",
    "\n",
    "with open(filepath, \"wb\") as file:\n",
    "    file.write(r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462eb3f7",
   "metadata": {},
   "source": [
    "Next we instantiate our wordpiece tokenizer with the vocabulary text file. We choose to transform tokens to strings but we can also directly obtain indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "627c98a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the wordpiece tokenizer with the vocabulary list (list of possible subwords)\n",
    "wp_tokenizer = tf_txt.WordpieceTokenizer(vocab_lookup_table='vocab.txt',\n",
    "                                         # can also be set to tf.int32 to obtain indices\n",
    "                                         token_out_type=tf.string) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03906e2e",
   "metadata": {},
   "source": [
    "We pre-process some text that we want to tokenize with the word piece tokenizer. For this we split it into words.\n",
    "\n",
    "We use a sentence that contains the word \"Westerberg\", which would be unlikely to be featured in a word-level vocabulary. Using sub-word tokenization, we can tokenize (and thus our model can also generate) words that are not part of the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "40fc143f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b'the' b'sunflowers' b'on' b'westerberg' b'remind' b'me' b'of'\n",
      " b'watching' b'the' b'sunset' b'in' b'my' b'hometown' b'.'], shape=(14,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "text = \"The sunflowers on Westerberg remind me of watching the sunset in my hometown.\".lower()\n",
    "\n",
    "# split text into words\n",
    "tokenizer = tf_txt.UnicodeScriptTokenizer()\n",
    "word_tokens = tokenizer.tokenize(text)\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac06131a",
   "metadata": {},
   "source": [
    "Finally we tokenize the words into sub-words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "104ae597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.RaggedTensor [[b'the'], [b'sun', b'##f', b'##low', b'##ers'], [b'on'], [b'west', b'##er', b'##berg'], [b'remind'], [b'me'], [b'of'], [b'watching'], [b'the'], [b'sun', b'##set'], [b'in'], [b'my'], [b'hometown'], [b'.']]>\n"
     ]
    }
   ],
   "source": [
    "sub_word_tokens = wp_tokenizer.tokenize(word_tokens)\n",
    "\n",
    "print(sub_word_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a1f3fa",
   "metadata": {},
   "source": [
    "The word \"westerberg\" is composed of three tokens \"west\", the suffix \"##er\", and the suffix \"##berg\". This type of tokenization is highly flexible which is one of the reasons it is used in most (if not all) modern language models.\n",
    "\n",
    "You may have noticed that the resulting tensor that we've printed above is not a normal tensor but a ragged tensor containing the words as rows of differing length. Ragged tensors can have elements of varying length without the need for padding. They are especially useful for RNN models because these can process sequences of varying length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0cfebe7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([14, None])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shape of a ragged tensor is None along the second dimension.\n",
    "sub_word_tokens.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f7b2a4",
   "metadata": {},
   "source": [
    "## Tokenization with SentencePiece (most versatile choice) \n",
    "\n",
    "The sentencepiece tokenization tool allows for tokenization without specifying at which level you want to tokenize. It allows for Byte-pair-encoding (BPE) which is used in BERT and GPT models (both are successful NLP models). SentencePiece does not need words to be split before tokenization, it can be used on raw text and thus also allows to tokenize many languages, even if they do not have white spaces or characters.\n",
    "\n",
    "Using the sentencepiece tokenizer will give you the most versatility and customizability while allowing for the use of state-of-the-art algorithms such as BPE (https://paperswithcode.com/method/bpe) and unigram (https://paperswithcode.com/method/unigram-segmentation). Both allow for regularization techniques (e.g. through sampling) on the tokenization side of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57365489",
   "metadata": {},
   "source": [
    "#### Training a sentencepiece model from scratch\n",
    "\n",
    "To train state of the art tokenizer models on your own text from scratch, you can use sentencepiece by Google (https://github.com/google/sentencepiece). It is recommended to train the model on a large corpus to learn useful sub-words.\n",
    "\n",
    "You will first have to install the Python package of sentencepiece."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9afdb6d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in /home/mp/anaconda3/envs/uni/lib/python3.8/site-packages (0.1.96)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e289c670",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "756c7dae",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: shakespeare_sonnets.txt\n",
      "  input_format: \n",
      "  model_prefix: tokenizer_model\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 512\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ‚Åá \n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(329) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(178) LOG(INFO) Loading corpus: shakespeare_sonnets.txt\n",
      "trainer_interface.cc(385) LOG(INFO) Loaded all 2177 sentences\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(405) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(466) LOG(INFO) all chars count=93717\n",
      "trainer_interface.cc(477) LOG(INFO) Done: 99.9552% characters are covered.\n",
      "trainer_interface.cc(487) LOG(INFO) Alphabet size=55\n",
      "trainer_interface.cc(488) LOG(INFO) Final character coverage=0.999552\n",
      "trainer_interface.cc(520) LOG(INFO) Done! preprocessed 2158 sentences.\n",
      "unigram_model_trainer.cc(139) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(143) LOG(INFO) Extracting frequent sub strings...\n",
      "unigram_model_trainer.cc(194) LOG(INFO) Initialized 7123 seed sentencepieces\n",
      "trainer_interface.cc(526) LOG(INFO) Tokenizing input sentences with whitespace: 2158\n",
      "trainer_interface.cc(537) LOG(INFO) Done! 4578\n",
      "unigram_model_trainer.cc(489) LOG(INFO) Using 4578 sentences for EM training\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=3058 obj=11.1231 num_tokens=9412 num_tokens/piece=3.07783\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=2658 obj=9.46876 num_tokens=9501 num_tokens/piece=3.57449\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=1992 obj=9.61747 num_tokens=10250 num_tokens/piece=5.14558\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=1989 obj=9.52743 num_tokens=10265 num_tokens/piece=5.16088\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=1491 obj=9.90731 num_tokens=11443 num_tokens/piece=7.67471\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=1491 obj=9.81248 num_tokens=11441 num_tokens/piece=7.67337\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=1118 obj=10.3528 num_tokens=12911 num_tokens/piece=11.5483\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=1118 obj=10.2437 num_tokens=12933 num_tokens/piece=11.568\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=838 obj=10.8072 num_tokens=14506 num_tokens/piece=17.3103\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=838 obj=10.7042 num_tokens=14504 num_tokens/piece=17.3079\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=628 obj=11.3255 num_tokens=16092 num_tokens/piece=25.6242\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=628 obj=11.2287 num_tokens=16100 num_tokens/piece=25.6369\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=563 obj=11.5722 num_tokens=16625 num_tokens/piece=29.5293\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=563 obj=11.5308 num_tokens=16625 num_tokens/piece=29.5293\n",
      "trainer_interface.cc(615) LOG(INFO) Saving model: tokenizer_model.model\n",
      "trainer_interface.cc(626) LOG(INFO) Saving vocabs: tokenizer_model.vocab\n"
     ]
    }
   ],
   "source": [
    "sp.SentencePieceTrainer.train(\n",
    "    input='shakespeare_sonnets.txt', model_prefix='tokenizer_model', model_type=\"unigram\", vocab_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0ca09c",
   "metadata": {},
   "source": [
    "The trained model is saved in a serialized format along with a vocabulary file. The vocabulary size here was set to be 512. Effectively this means that single characters as well as sub-words and whole words are part of the vocabulary. In some cases even phrases can become a single token since the segmentation model does not discriminate between words, subwords and collections of words like word-piece."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f8defeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deserialize the trained model file to load it in the correct format\n",
    "trained_tokenizer_model = tf.io.gfile.GFile('tokenizer_model.model', \"rb\").read()\n",
    "\n",
    "# load the model as a tokenizer that can be used inside a tensorflow model\n",
    "tokenizer = tf_txt.SentencepieceTokenizer(\n",
    "    model=trained_tokenizer_model, out_type=tf.int32, nbest_size=-1, alpha=1, reverse=False,\n",
    "    add_bos=False, add_eos=False, return_nbest=False, name=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6c597da5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 39 148  49   3 437], shape=(5,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(\"thou shall not pass\")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5959e6ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b'thou shall not pass'>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.detokenize(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539cb2c6",
   "metadata": {},
   "source": [
    "Next we can also see how many tokens the sentencepiece model trained on shakespeare's sonnets needs to represent the word \"Westerberg\". Running the following code cell multiple times shows us that tokenization involves stochasticity, which is supposed to help regularize the optimization of language models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "272cdd9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'we', shape=(), dtype=string)\n",
      "tf.Tensor(b'st', shape=(), dtype=string)\n",
      "tf.Tensor(b'er', shape=(), dtype=string)\n",
      "tf.Tensor(b'b', shape=(), dtype=string)\n",
      "tf.Tensor(b'er', shape=(), dtype=string)\n",
      "tf.Tensor(b'g', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(\"westerberg\")\n",
    "\n",
    "for token in tokens:\n",
    "    print(tokenizer.detokenize([token]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846d0a96",
   "metadata": {},
   "source": [
    "Lastly, Sentencepiece can also be used as a standalone package (e.g. when working in a different Deep Learning framework such as PyTorch or JAX)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a4e5ad20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the SentencePieceProcessor\n",
    "tokenizer = sp.SentencePieceProcessor(model_file=\"tokenizer_model.model\",out_type=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3d06e638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[248, 148, 49, 3, 437]\n"
     ]
    }
   ],
   "source": [
    "# Encode text with the tokenizer\n",
    "tokenized_text = tokenizer.encode(\"Thou shall not pass\", out_type=int)\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5437c5f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Thou shall not pass'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Decode the list of indices with the tokenizer\n",
    "tokenizer.decode(tokenized_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:uni] *",
   "language": "python",
   "name": "conda-env-uni-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
